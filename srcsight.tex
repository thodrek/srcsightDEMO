% THIS IS AN EXAMPLE DOCUMENT FOR VLDB 2012
% based on ACM SIGPROC-SP.TEX VERSION 2.7
% Modified by  Gerald Weber <gerald@cs.auckland.ac.nz>
% Removed the requirement to include *bbl file in here. (AhmetSacan, Sep2012)
% Fixed the equation on page 3 to prevent line overflow. (AhmetSacan, Sep2012)

\documentclass{vldb}
\usepackage{graphicx}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{cleveref}
\usepackage{times}

\newcommand\system{\textsc{SourceSight}}
\newtheorem{definition}{Definition}


\begin{document}

% ****************** TITLE ****************************************

\title{{\LARGE \system}: Enabling Effective Source Selection}

\numberofauthors{3} 

\author{
% 1st. author
\alignauthor
Theodoros Rekatsinas\\
       \affaddr{University of Maryland}\\
       \email{thodrek@cs.umd.edu}
% 2nd. author
\alignauthor
Amol Deshpande\\
       \affaddr{University of Maryland}\\
       \email{amol@cs.umd.edu}
% 3rd. author
\alignauthor 
Xin Luna Dong\\
       \affaddr{Google Inc.}\\
       \email{lunadong@google.com}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
Lise Getoor\\
       \affaddr{UC Santa Cruz}\\
       \email{getoor@soe.ucsc.edu}
% 5th. author
\alignauthor Divesh Srivastava\\
       \affaddr{AT\& T Labs-Research}\\
       \email{divesh@research.att.com}
}

\maketitle

\begin{abstract}
Recently there has been a rapid increase in the number of data sources and publicly accessible data services, such as cloud-based data markets and data portals, that facilitate the collection, publishing and trading of data. Data sources typically exhibit large heterogeneity in the type and quality of data they provide. Unfortunately, when the number of data sources is large, users have a limited capability of reasoning about the actual usefulness of sources and the trade-offs between the benefits and costs of acquiring and integrating sources. In this demonstration we present \system, a framework that facilitates the exploration and selection of sources for integration. \system~is based on the intuition that different sets of data sources are optimal to integrate for different integration tasks. The main focus of \system~is to allow users to discover and explore valuable sets of sources for diverse integration tasks over large numbers of heterogeneous data sources.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Data integration remains among the most cost-intensive tasks in data management, either because of the considerable computation and human-curation costs involved in the process~\cite{kruse2015estimating}, or the monetary cost involved in acquiring data~\cite{balazinska:vldb11}. Given the high number of available data sources and the aforementioned costs, it is challenging for a user to identify the sources that are truly beneficial to her application. This gives rise to the natural question of how can one discover {\em the most beneficial sources} for integration, i.e., sources that maximize the user's benefit at the minimum cost. 

Several approaches have been proposed to help users reason about the value of integrating multiple data sources. The proposed approaches can be divided into two main categories: (i) {\em effort-oriented} techniques that focus on characterizing solely the cost of integration, and (ii) {\em benefit-oriented} techniques that reason about the benefit of integration. Techniques from the first category estimate the cost of integration via reasoning about the effort required to perform schema-matching, data-cleaning and data-transformation when integrating multiple sources~\cite{kruse2015estimating, smith:2009}. While valuable at estimating the overall cost of integration, these techniques do not reason about the actual benefits of integrating multiple sources. On the other hand, benefit-oriented techniques put more emphasis on characterizing the benefit of integrating a new source based on its marginal gain~\cite{dong:vldb13,rekatsinas:2014}. Nevertheless, both approaches usually assume that users have already identified all the sources relevant to their application and do not allow users to explore and identify which sources are the most beneficial for their desired {\em integration task}. Moreover, they do not support diverse integration tasks across multiple users.

To understand these limitations, consider the scenario of biologists that want to combine diverse sources of information from functional genomics experiments to make large-scale predictions for protein-to-protein interaction networks. Each prediction task is associated with an {\em integration task} that is characterized by the organism and the genes the biologist is targeting. Results of such experiments are usually collected in repositories such as ArrayExpress\footnote{http://www.ebi.ac.uk/arrayexpress/}. However, such repositories contain vast amounts of experimental data (e.g., ArrayExpress has results from 56,369 different experiments) and while users can provide a description of their integration task (i.e., a description of the genes they are interested) they might not know all the experiments that are relevant to their task. Therefore it is of great importance to allow users to discover and select sources based purely based on their integration task description. Moreover, such repositories are not limited to specific experiments but are designed to support diverse tasks posed by multiple users. For example, one group of users might be interested in human protein-to-protein interaction networks ~\cite{humanpp} and others in yeast protein-to-protein networks~\cite{yeastpp}.

Recently, we introduced our vision for a {\em data source management system}~\cite{rekatsinas:2015} that enables users to discover the most valuable sources for their applications and described a preliminary system design that can support diverse integration scenarios and can help users truly understand the quality and cost trade-offs between different integration options. In this demonstration proposal, we present \system, a fully functional prototype of such a data source management system. We also discuss extensions to our initial design that allow user to evaluate the effectiveness of the system (\Cref{sec:extensions}). The core of the system is built around the paradigm of {\em source selection}~\cite{dong:vldb13} that allows one to reason about the trade-offs between the gain and cost of integration. The gain of integration can be quantified using a variety of data quality metrics, such as coverage, accuracy, timeliness and bias. The cost of integration is quantified using a model similar to that of Dong et al.~\cite{dong:vldb13}, however, other cost models such as the one introduced by Kruse et al.~\cite{kruse2015estimating} can be seamlessly incorporated in our system. \system~offers a number of unique features:

\vspace{2pt}\noindent (1) Users can describe the domain of their desired integration task using a keyword-based interface. Moreover, the system allows them to explore similar or related domains refining the description of their desired task. Simultaneously users have the opportunity to explore sources that are highly relevant to their desired task.

\vspace{2pt}\noindent (2) Given a desired integration task, users can perform source selection using a variety of quality metrics. \system~casts source selection as a multi-variate optimization problem to help users understand the actual trade-offs between different quality metrics and proposes multiple sets of sources to be used when different weights are assigned to different quality metrics. 

\vspace{2pt}\noindent (3) Finally, the system allows users to edit the recommended solutions by adding or deleting sources. Users can also perform a qualitative comparison between different sets of sources. This is crucial for evaluating the solutions recommended by \system~and aids the users to understand why the particular set of sources was proposed by the system.

In the following sections, we describe the organization of the demonstration. First, we review what it means for a user to discover valuable sources for a specified integration task (\Cref{sec:sources}). Then we present an overview of the architecture of \system~and describe its core functionalities and discuss how they enable effective source selection for diverse integration tasks (\Cref{sec:design}). Finally, we provide an outline of the \system~use cases that VLDB attendees will experience during the demo (\Cref{sec:details}).

\vspace{2pt}\noindent\textbf{Related work.}

\section{Discovering Valuable Sources}
\label{sec:sources}
Consider a scenario where users interact with a repository that stores the raw data from multiple sources and want to identify sources relevant to a desired integration task. In \Cref{sec:intro}, we presented an example of biologists interacting with ArrayExpress but other use cases include scientists that share their experimental results and collaborate on large-scale datasets via a repository such as DataHub~\cite{datahub}, or business analysts that want to acquire data from a marketplace~\cite{balazinska:vldb11} or journalists that want to write an overview article and want to compile information from multiple news sources stored in a single repository such as EventRegistry\footnote{http://eventregistry.org/}. 

As discussed previously, users may not know all sources that are relevant to their integration task. However, it is safe to assume that they can provide us with a keyword-based description of the domain that is relevant to their task. Based on this description our goal is twofold: 

\begin{figure}
	\begin{center}
	\includegraphics[trim=0 0 0 60, clip,width=0.48\textwidth]{fig/exploreCor}
	\caption{Discovering relevant sources and refining an integration task with \system.}
	\label{fig:exploration}
	\end{center}
	\vspace{-20pt}
\end{figure}


First, we want to enable users to {\em discover} sources relevant to the provided integration task description but also allow them to {\em refine} their description by exploring related tasks. For example, a journalist that wants to write an overview article about the socio-economic situation in Greece may start by requesting news sources relevant to the keyword ``Greece''. However, it might be beneficial to explore related and potentially more specialized descriptions that are related with ``Greece'' (i.e., ``Greece and Finance'' or ``Greece and Human Interest'') as the set of relevant sources may be significantly different. \system~is the first system that unifies source exploration with task integration refinement under a common interface (see \Cref{fig:exploration}).

Second, the data sources discovered in the previous step may exhibit large heterogeneity with respect to the data they provide. They may provide stale or erroneous data~\cite{Dong_vldb:2009, li:2012}, they may contain duplicate data~\cite{bronzi:2013, li:2012} at different prices, and may exhibit schema or data instance heterogeneity (e.g., provide synonym mentions to the same real-world entities). Given the heterogeneity of sources we want to enable users to discover sets of sources that if integrated together will maximize the {\em benefit} of integration under any effort or cost constraints the user may have. For example the journalist mentioned above may want to review a limited number of news sources due to time constraints. 

To support this, \system~allows users to perform {\em source selection} over the set of sources $S$ that are deemed relevant to their integration task description. Source selection assumes an oracle $G(\cdot)$ that takes as input a set of sources $\bar{S}$ and the description of an integration task $I_d$ and returns the gain of integrating sources $\bar{S}$. It also assumes an oracle $C(\cdot)$ that takes as input a set of sources $\bar{S}$ and returns the total integration cost. The result of source selection corresponds to a sets of sources $S_I$ such that $S_I = \arg\max_{\bar{S} \subseteq S}G(\bar{S},I_d) - C(\bar{S})$. To quantify the gain of integration, \system~uses a range of rigorous data quality metrics, such as {\em coverage}, {\em accuracy}, {\em timeliness} or {\em bias}~\cite{dong:vldb13,rekatsinas:2014,rekatsinas:2015}. 

\begin{figure}
	\begin{center}
	\includegraphics[trim=0 0 0 85, clip,width=0.48\textwidth]{fig/ssResults}
	\caption{\system's interface for exploring different source selection solutions.}
	\label{fig:ssresults}
	\end{center}
	\vspace{-20pt}
\end{figure}

Given this variety of quality metrics, \system~provides the user with multiple source selection solutions that correspond to different weighting configurations for the available quality metrics. This allows users to explore different trade-offs amongst the available quality metrics and identify the set of sources that best satisfies their quality requirements (see \Cref{fig:ssresults}).

\section{{\Large \system} Design}
\label{sec:design}
In a recent paper~\cite{rekatsinas:2015}, we put forward our vision about the functionalities that data source management systems should support to facilitate effective source selection and the architecture they should follow. 
\system~is a realization of our vision for such a system. Next, we describe \system's architecture, the different functionalities it supports and extensions of the preliminary design implemented in \system.%

\subsection{Architecture overview}
\system~can be built as a layer on top of a data source repository that stores the raw data of data sources (see \Cref{sec:sources}). The system consists of three components: a frontend, a source analysis engine and an exploration engine. The frontend is a ``thin client'' that is used to specify integration tasks and display visualizations that aid users to select the desired sources for integration. The source analysis engine processes the raw data from the available data sources to identify the content of sources, and also evaluates their quality with respect to different quality metrics such as the ones described in the previous section. Finally, the exploration engine takes as input user-specified requests and performs the necessary computation to serve those. The architecture of \system~is presented in \Cref{fig:architecture}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/srcsightOver}
    \caption{\system~architecture.}
    \label{fig:architecture}
\end{figure}

The basic operations of \system~can be divided into an {\em offline} phase and an {\em online} phase. At the offline phase, the source analysis module constructs an index describing both the content and the quality of each source with respect to multiple quality metrics. This index is used during the online phase when a user interacts with the system to serve the user requests. We discuss the indexing technique in more detail in \Cref{sec:reasoning}. At the online phase users interact with \system~via its frontend module and user requests are served by the query engine. Users can specify a desired integration task by providing a keyword-based description of their integration task. Once a description is provided users can choose among three main functionalities with respect to the specified integration task. They can (i) choose to explore which keywords and sources are highly relevant to the specified task, (ii) choose to perform source selection, or (iii) choose to perform a qualitative comparison between different sets of sources constructed either manually or automatically via source selection. The first functionality allows a user to refine her integration task specification by exploring related contexts. It also allows users to obtain an overview of the quality of individual sources (see \Cref{sec:integtask}). The second functionality returns multiple recommended source selection solutions with respect to a variety of quality metrics (see \Cref{sec:sourcesel}). Finally, \system~allows users to compare and contrast different sets of sources to further understand the benefits of choosing a specific set and understand why the particular set was recommended by source selection (see \Cref{sec:extensions}). We now discuss the different components in detail.

\subsection{Reasoning about the content of sources}
\label{sec:reasoning}
We assume that each entry in a source is associated with a set of {\em context literals} and all context literals come from a {\em literal dictionary} $V$. For example, these literals may correspond to {\em real-world entities} or {\em concepts}. In addition, when the source entry is structured, additional literals may correspond to attributes of each entry. The aforementioned assumptions allow one to reason about the domains covered by each source by analyzing the union of context-literal sets associated with the entries of the source. 

We also consider that $V$ follows a structured form, i.e., corresponds to a {\em knowledge base} (or ontology) describing how different literals are related to each other. To reason about the content of different sources and their data quality we recently proposed augmenting such a knowledge base with a {\em correspondence graph}~\cite{rekatsinas:2015}. 
\begin{figure}[h]
	\begin{center}
	\includegraphics[clip,scale=0.25]{fig/kgcg}
	\caption{An example of a knowledge base and a correspondence graph with two c-cluster nodes corresponding to the population of countries in Asia and sports in the USA.}
	\label{fig:kgcg}
	\vspace{-10pt}
	\end{center}
\end{figure}

Specifically, the nodes in the correspondence graph are either data sources (referred to as {\em source nodes}) or clusters of concepts and/or entities as dictated by the available sources (called {\em c-cluster nodes}). The edges in the correspondence graph connect each source node with c-cluster nodes and c-cluster nodes with the corresponding concepts and entities in the knowledge base.  The edges connecting c-cluster nodes to concepts and entities follow conjunctive semantics. Each edge from a source to a c-cluster node is annotated with a quality profile of that source for that specific c-cluster, and each c-cluster node is associated with local information about the dependencies of the data sources that are connected to it. An example of a knowledge base and a correspondence graph is shown in \Cref{fig:kgcg}. There are two c-cluster nodes, one corresponding to the population of countries in Asia and one to sports in the USA (i.e., ``USA and Sports''). 

The correspondence graph serves as a content and quality index for the available sources. To construct it we first learn the latent c-cluster nodes and then compute the quality profiles and data source dependencies for each c-cluster node. To discover the literals associated with each source entry we use Thomson Reuter's Open Calais\footnote{http://www.opencalais.com/}, an API for semantic annotations with respect to multiple knowledge bases including DBpedia, Freebase and others. To construct the c-cluster nodes we adopt a frequent pattern mining approach based on the FP-growth algorithm~\cite{Han:2000}. This approach allows us to discover domains that are prevalent in multiple sources. After discovering the c-cluster nodes, we compute the quality of each source with respect to each c-cluster node it is connected to. For the latter, we collectively analyze the content of all sources connected to a c-cluster node following an approach similar to Rekatsinas et al.~\cite{rekatsinas:2014} where samples from all the sources are integrated into a single dataset forming the content of the c-cluster and then each individual sample is compared with the integrated data to compute the source quality. 

\subsection{Specifying and Refining an Integration Task}
\label{sec:integtask}
\system-allows users to specify an integration task $I = (I_d, I_c)$ by providing a description that corresponds to a context-literal set $I_d$ defining the {\em domain} of the task and potentially a set of integration cost or effort constraints $I_c$. The system offers users with the functionality of exploring literals and sources that are related to their keyword search enabling them to explore similar integration tasks or refine their initial task. 

Relevant context-literals to $I_d$ are all literals that are related to literals in $I_d$ either via $V$ or via the constructed correspondence graph. The former accounts for semantic relations across literals while the second for co-occurrence of literals. Similarly, relevant sources to task $I$ are all data sources that provide entries whose context-literal set is a subset of $I_d$ or related to $I_d$ via the literal dictionary $V$ (e.g., using equivalence or containment relationships). 

\system~offers a unified interface for users to explore both context-literals and sources related to their desired integration task (see \Cref{fig:exploration}). Given an integration task description $I_d$, \system~returns the set of top relevant literals to the search of the user as well as the most relevant sources with respect to coverage for the specified keyword search. The user can then select any of the recommended sources to view a summary of the literals that the source covers as well as a quality summary of the source for the corresponding keyword search. Users can also choose to update their integration task description $I_d$ by including new relevant context-literals to $I_d$.

\subsection{Multi-variate Source Selection}
\label{sec:sourcesel}
Once a user has identified the context-literal description that fully specifies her integration task, she can perform source selection with respect to that set of literals and her constraints $I_c$. As mentioned in \Cref{sec:sources}, source selection considers multiple quality metrics to quantify the benefit of integration. \system~casts source selection as a multi-objective optimization problem and the query engine finds the set of {\em Pareto optimal} solutions corresponding to the source selection problem at hand. 

Discovering all the solutions on the Pareto front is expensive as one needs to reason about all the potential trade-offs amongst the available quality metrics. To address this issue we use a {\em sampling} strategy to recover solutions that correspond to different quality trade-offs. More precisely, let $Q$ is the set of quality metrics under consideration and $G_q(\cdot)$ be an oracle computing the gain of integration with respect to quality metric $q \in Q$ for any set of sources. We compute the total gain of integration as a weighted linear combination of the individual gain for each quality metric, i.e., $G(\cdot) = \sum_{q \in Q} w_q \cdot G_q(\cdot)$. Given this definition of the total gain of integration, we sample different combinations for the weights $w_q$ and solve source selection for each of those. Finally we identify the Pareto optimal solutions amongst the sampled solutions. 

All the sampled solutions are presented to the user in a way that makes it easy for her to compare the quality of each solution. The corresponding interface is shown in \Cref{fig:ssresults}. Users can select a particular solution and view a concise summary of the gain and cost of integration achieved by it. Moreover,  users have the flexibility to drill down and expand only on a subset of the available quality metrics. This allows them to fully understand specific trade-offs across different solutions. Finally, users can view a more detailed description of a proposed source selection solution including information such as the sources included in the result and their individual contributions to the quality of the final integration result. 

\subsection{Comparing and Contrasting Sets of Sources}
\label{sec:extensions}
Both functionalities presented above were alluded to our visionary paper on data source management systems~\cite{rekatsinas:2015}. Next, we discuss an analytical functionality in \system~that enables users to understand why a particular set of sources was recommended to them and evaluate the performance of the system.

\system~allows users to perform a qualitative comparison between two sets of sources with respect to the same integration task. The corresponding interface is shown in \Cref{fig:comparison}. In particular, users will be able to examine how the two sets of sources compare against each other with respect to individual quality metrics, as well as the total integration gain and cost. The aforementioned sets of source can either be selected directly from the solutions of source selection or can be manually constructed by the user. \system~offers two ways of manually specifying sets of sources: (i) users can start from a running source selection solution and add new sources or remove sources already included in the set, or (ii) users can manually select different sources and construct their own set. The former allows them to examine the neighborhood of the corresponding solution but also serves as a mechanism of convincing the user of the benefit achieved by the output of source selection. To enable the latter, \system~provides users with the top-k most relevant sources with respect to their integration task. As users select sources the ranking is revised to contain sources that are more beneficial given the current selection of the user. We point out that the rankings are computed with respect to the individual additional benefit of each source. 
\begin{figure}
	\begin{center}
	\includegraphics[trim=0 0 0 0, clip,width=0.48\textwidth]{fig/compSS}
	\caption{Comparing sets of sources.}
	\label{fig:comparison}
	\end{center}
\end{figure}

\section{Demo Details}
\label{sec:details}
We will demonstrate the functionality of \system~through hands-on experience with two large scale real-world datasets. The data will be stored on a remote server and users will interact with the system via a web interface. Our goals are two fold: (i) demonstrate the utility of \system~in exploring and selecting beneficial sources for integration and (ii) demonstrate the effectiveness of automatic source selection techniques for diverse scenarios. 

\vspace{2pt}\noindent\textbf{Datasets.} The first dataset we will use corresponds to event extractions collected from EventRegistry\footnote{http://eventregistry.org/}, a repository that monitors news media from all over the world and extracts geo-referenced records that correspond to different news articles. Data sources correspond to different news domains and quality metrics such as coverage, timeliness or position bias of the articles are inherently relevant. EventRegistry gets updated at fixed intervals  by ingesting feeds of newly extracted news articles. We believe that EventRegistry is the right fit to demonstrate the usefulness and practicality of \system~due to the large number of data sources available, the available updates over time, and the heterogeneity that the sources exhibit both with respect to their domain and their quality. We plan to use a recent snapshot retrieved from EventRegistry containing at least six months of news article data. 

The second dataset we will use corresponds to a Twitter snapshot. Here, individual Twitter users can be viewed as sources. Twitter gives its users an unprecedented ability to deliver news as developments unfold and has been adopted by many researchers as the means of predicting occurrences of events in the future. Hence, we believe that analyzing collections of tweets via \system~to identify sets of expert twitter users by employing source selection techniques would be of great interest to the VLDB participants.

\vspace{2pt}\noindent\textbf{Demonstrating Utility.} Attendees will be able to describe ad-hoc or pre-formulated integration tasks in \system. Then they will have the opportunity to evaluate the source exploration and source selection functionalities offered by \system~(described in \Cref{sec:integtask} and \Cref{sec:sourcesel}) with respect to these tasks. Our goal is for users to understand the trade-offs between different quality metrics of sources in both datasets we have described above and understand how \system~can guide them to select the most suitable set of sources for their application needs.

\vspace{2pt}\noindent\textbf{Demonstrating Effectiveness.} For this part of the demo users will mainly interact with the third functionality of \system, i.e., comparing and contrasting sets of sources. Attendees will have the opportunity to compare sets of sources provided by source selection with manually created sets of sources and understand the quality differences across them. To facilitate manual selection of sources, we will provide users with ranking mechanisms that consider each source in isolation for each individual quality metric. Starting from the recommended ranked sources users will be able to form their own set of sources by adding or removing sources and compare it with sets generated by \system. 

Overall, the demo will allow users to: (i) understand the internal source indexing mechanism (i.e., the correspondence graph) of our prototype system, (ii) issue queries against it and (iii) explore the corresponding source selection solutions via a web-interface. Users will need to play the role of an analyst and use our system to discover the most valuable sources for their own analysis applications.

{\small 
\bibliographystyle{abbrv}
\bibliography{srcsight}}

\end{document}
